{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gptahmed1/-ZenAI-Assistant-Bot/blob/main/open_deep_researcher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7cTpP9rDZW-",
        "outputId": "301df9d1-83d3-415e-9ba0-c4a93a134960"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJTo96a7DGUz",
        "outputId": "7e05e254-0a77-4730-9a7d-b03a61415e58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "أدخل استعلام البحث أو الموضوع: ai news\n",
            "أدخل الحد الأقصى للتكرارات (افتراضي 10): \n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "هذا الكود عبارة عن تطبيق بحث متكامل يعتمد على Gemini API وواجهات خارجية مثل SERPAPI وJina.\n",
        "المهمة الأساسية هي تحسين الأداء والسرعة والاستجابة مع الحفاظ على نفس المزايا الأساسية.\n",
        "تم تطوير الكود دون إضافة أي مزايا جديدة، مع التركيز على تعزيز الوظائف الحالية وتفادي المشاكل.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import json\n",
        "import logging\n",
        "import google.generativeai as genai\n",
        "from functools import wraps\n",
        "\n",
        "# إعداد سجل الأخطاء والتتبع (logging)\n",
        "logging.basicConfig(\n",
        "    level=logging.DEBUG,\n",
        "    format='[%(asctime)s] %(levelname)s - %(message)s',\n",
        "    datefmt='%Y-%m-%d %H:%M:%S'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ===========================================================\n",
        "# تعيين مفتاح GEMINI_API_KEY مباشرة دون استخدام متغيرات البيئة\n",
        "# ===========================================================\n",
        "GEMINI_API_KEY = \"AIzaSyBWIPKFRONUbaskpwLgSpx-KI61Bi0LikY\"\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# ===========================================================\n",
        "# إعداد تكوين النموذج الذي سيتم استخدامه في الاستجابة\n",
        "# ===========================================================\n",
        "generation_config = {\n",
        "    \"temperature\": 1,\n",
        "    \"top_p\": 0.95,\n",
        "    \"top_k\": 40,\n",
        "    \"max_output_tokens\": 8192,\n",
        "    \"response_mime_type\": \"text/plain\",\n",
        "}\n",
        "\n",
        "# إنشاء النموذج باستخدام Gemini API (يعتمد فقط على gemini-1.5-pro)\n",
        "model = genai.GenerativeModel(\n",
        "    model_name=\"gemini-1.5-pro\",\n",
        "    generation_config=generation_config,\n",
        ")\n",
        "\n",
        "# ===========================================================\n",
        "# دالة مساعدة لتسجيل الزمن الزمني لأداء الدوال (Decorator)\n",
        "# ===========================================================\n",
        "def timed(func):\n",
        "    @wraps(func)\n",
        "    async def wrapper(*args, **kwargs):\n",
        "        start_time = asyncio.get_event_loop().time()\n",
        "        result = await func(*args, **kwargs)\n",
        "        end_time = asyncio.get_event_loop().time()\n",
        "        logger.debug(f\"تنفيذ الدالة '{func.__name__}' استغرق {end_time - start_time:.4f} ثانية.\")\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "# ===========================================================\n",
        "# دالة إرسال رسالة باستخدام Gemini API\n",
        "# ===========================================================\n",
        "def gemini_send_message(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    ترسل الرسالة إلى Gemini API وتعيد نص الاستجابة.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # بدء جلسة دردشة جديدة مع التاريخ الفارغ\n",
        "        chat_session = model.start_chat(history=[])\n",
        "        response = chat_session.send_message(prompt)\n",
        "        logger.debug(\"تم إرسال الرسالة بنجاح إلى Gemini API.\")\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        logger.error(\"خطأ أثناء إرسال الرسالة إلى Gemini API:\", exc_info=True)\n",
        "        return \"\"\n",
        "\n",
        "# ===========================================================\n",
        "# دالة استدعاء OpenRouter (باستخدام Gemini API) بشكل غير متزامن\n",
        "# ===========================================================\n",
        "@timed\n",
        "async def call_openrouter_async(session: aiohttp.ClientSession, messages: list) -> str:\n",
        "    \"\"\"\n",
        "    تجمع جميع الرسائل في محادثة واحدة وتستدعي Gemini API.\n",
        "    تُستخدم هذه الدالة لاستدعاء نموذج Gemini بطريقة غير متزامنة.\n",
        "    \"\"\"\n",
        "    # تجميع الرسائل في نص واحد، مع الحفاظ على ترتيب الأدوار\n",
        "    conversation = \"\"\n",
        "    for m in messages:\n",
        "        conversation += f\"{m['role']}: {m['content']}\\n\"\n",
        "    try:\n",
        "        # استدعاء Gemini API بطريقة غير متزامنة باستخدام asyncio.to_thread لتجنب حظر الحلقة\n",
        "        result = await asyncio.to_thread(gemini_send_message, conversation)\n",
        "        if not result:\n",
        "            logger.warning(\"استجابة فارغة من Gemini API عند استدعاء call_openrouter_async.\")\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        logger.error(\"خطأ في call_openrouter_async:\", exc_info=True)\n",
        "        return \"\"\n",
        "\n",
        "# ===========================================================\n",
        "# الثوابت الخاصة بواجهات البحث الخارجية\n",
        "# ===========================================================\n",
        "# تم إزالة مفتاح SERPAPI_API_KEY واعتماده على API مجاني (أو عدم الحاجة إليه)\n",
        "SERPAPI_API_KEY = \"\"  # API مجاني غير مطلوب\n",
        "SERPAPI_URL = \"https://serpapi.com/search\"  # يبقى الرابط كما هو ولكن لن يتم إرسال مفتاح API\n",
        "JINA_API_KEY = \"jina_b331108c76be465fa1dbe1e913cccaaeqsGnJmuKYHnC1J4j4c1L7ogD671l\"     # استبدل بمفتاح JINA الخاص بك\n",
        "\n",
        "# ===========================================================\n",
        "# دوال مساعدة للتعامل مع عمليات البحث والبيانات\n",
        "# ===========================================================\n",
        "\n",
        "@timed\n",
        "async def generate_search_queries_async(session: aiohttp.ClientSession, user_query: str) -> list:\n",
        "    \"\"\"\n",
        "    توليد استعلامات بحث دقيقة بناءً على استعلام المستخدم.\n",
        "    يتم طلب من نموذج Gemini توليد قائمة بايثون من السلاسل النصية.\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        \"أنت مساعد بحث خبير. بناءً على استعلام المستخدم، أنشئ حتى أربعة \"\n",
        "        \"استعلامات بحث دقيقة ومتميزة للحصول على معلومات شاملة حول الموضوع. \"\n",
        "        \"أعد قائمة بايثون من السلاسل النصية فقط، مثل: ['استعلام1', 'استعلام2'].\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"أنت مساعد بحث مفيد ودقيق.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"استعلام المستخدم: {user_query}\\n\\n{prompt}\"}\n",
        "    ]\n",
        "    response = await call_openrouter_async(session, messages)\n",
        "    if response:\n",
        "        try:\n",
        "            # التحقق من أن الاستجابة عبارة عن قائمة بايثون باستخدام eval مع فحص نوع النتيجة\n",
        "            search_queries = eval(response)\n",
        "            if isinstance(search_queries, list):\n",
        "                logger.debug(\"تم توليد استعلامات البحث بنجاح.\")\n",
        "                return search_queries\n",
        "            else:\n",
        "                logger.warning(\"الرد لم يكن قائمة: الرد = %s\", response)\n",
        "                return []\n",
        "        except Exception as e:\n",
        "            logger.error(\"خطأ في تحليل استعلامات البحث باستخدام eval:\", exc_info=True)\n",
        "            return []\n",
        "    logger.warning(\"لم يتم الحصول على استجابة من Gemini لتوليد استعلامات البحث.\")\n",
        "    return []\n",
        "\n",
        "@timed\n",
        "async def perform_search_async(session: aiohttp.ClientSession, query: str) -> list:\n",
        "    \"\"\"\n",
        "    تنفيذ بحث على جوجل باستخدام SERPAPI بناءً على الاستعلام.\n",
        "    تعيد الدالة قائمة من الروابط التي تم استخراجها.\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        \"q\": query,\n",
        "        \"engine\": \"google\"\n",
        "    }\n",
        "    # إضافة مفتاح API فقط إذا كان غير فارغ\n",
        "    if SERPAPI_API_KEY:\n",
        "        params[\"api_key\"] = SERPAPI_API_KEY\n",
        "    try:\n",
        "        async with session.get(SERPAPI_URL, params=params) as resp:\n",
        "            if resp.status == 200:\n",
        "                results = await resp.json()\n",
        "                if \"organic_results\" in results:\n",
        "                    links = [item.get(\"link\") for item in results[\"organic_results\"] if \"link\" in item]\n",
        "                    logger.debug(\"تم استخراج %d رابط من SERPAPI.\", len(links))\n",
        "                    return links\n",
        "                else:\n",
        "                    logger.warning(\"لا توجد نتائج عضوية في رد SERPAPI.\")\n",
        "                    return []\n",
        "            else:\n",
        "                text = await resp.text()\n",
        "                logger.error(\"خطأ SERPAPI: %d - %s\", resp.status, text)\n",
        "                return []\n",
        "    except Exception as e:\n",
        "        logger.error(\"استثناء أثناء تنفيذ بحث SERPAPI:\", exc_info=True)\n",
        "        return []\n",
        "\n",
        "@timed\n",
        "async def fetch_webpage_text_async(session: aiohttp.ClientSession, url: str) -> str:\n",
        "    \"\"\"\n",
        "    جلب محتوى الصفحة النصي باستخدام Jina.\n",
        "    يتم استخدام رابط URL المركب لطلب المحتوى.\n",
        "    \"\"\"\n",
        "    full_url = f\"{JINA_BASE_URL}{url}\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {JINA_API_KEY}\"\n",
        "    }\n",
        "    try:\n",
        "        async with session.get(full_url, headers=headers) as resp:\n",
        "            if resp.status == 200:\n",
        "                text_content = await resp.text()\n",
        "                logger.debug(\"تم جلب محتوى الصفحة بنجاح من %s.\", url)\n",
        "                return text_content\n",
        "            else:\n",
        "                text = await resp.text()\n",
        "                logger.error(\"خطأ جلب الصفحة %s: %d - %s\", url, resp.status, text)\n",
        "                return \"\"\n",
        "    except Exception as e:\n",
        "        logger.error(\"استثناء أثناء جلب محتوى الصفحة باستخدام Jina من %s:\", url, exc_info=True)\n",
        "        return \"\"\n",
        "\n",
        "@timed\n",
        "async def is_page_useful_async(session: aiohttp.ClientSession, user_query: str, page_text: str) -> str:\n",
        "    \"\"\"\n",
        "    تقييم مدى فائدة محتوى الصفحة للإجابة على استعلام المستخدم.\n",
        "    يجب على النموذج الرد بكلمة \"Yes\" أو \"No\" فقط.\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        \"أنت مقيم بحث نقدي. بناءً على استعلام المستخدم ومحتوى الصفحة، \"\n",
        "        \"حدد ما إذا كانت الصفحة تحتوي على معلومات مفيدة للإجابة. \"\n",
        "        \"أجب بكلمة واحدة بالضبط: 'Yes' أو 'No' دون أي نص إضافي.\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"أنت مقيم بحث صارم ومختصر.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"استعلام المستخدم: {user_query}\\n\\nمحتوى الصفحة (أول 20000 حرف):\\n{page_text[:20000]}\\n\\n{prompt}\"}\n",
        "    ]\n",
        "    response = await call_openrouter_async(session, messages)\n",
        "    if response:\n",
        "        answer = response.strip()\n",
        "        if answer in [\"Yes\", \"No\"]:\n",
        "            logger.debug(\"تقييم فائدة الصفحة: %s\", answer)\n",
        "            return answer\n",
        "        else:\n",
        "            if \"Yes\" in answer:\n",
        "                logger.debug(\"تم استخراج 'Yes' من الإجابة غير الدقيقة.\")\n",
        "                return \"Yes\"\n",
        "            elif \"No\" in answer:\n",
        "                logger.debug(\"تم استخراج 'No' من الإجابة غير الدقيقة.\")\n",
        "                return \"No\"\n",
        "    logger.warning(\"فشل تقييم فائدة الصفحة، إعادة القيمة الافتراضية 'No'.\")\n",
        "    return \"No\"\n",
        "\n",
        "@timed\n",
        "async def extract_relevant_context_async(session: aiohttp.ClientSession, user_query: str, search_query: str, page_text: str) -> str:\n",
        "    \"\"\"\n",
        "    استخراج المعلومات ذات الصلة من محتوى الصفحة بناءً على استعلام المستخدم واستعلام البحث.\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        \"أنت مستخرج معلومات خبير. بناءً على استعلام المستخدم، واستعلام البحث المستخدم، \"\n",
        "        \"ومحتوى الصفحة، استخرج كل المعلومات ذات الصلة للإجابة دون تعليق إضافي.\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"أنت خبير في استخراج وتلخيص المعلومات.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"استعلام المستخدم: {user_query}\\nاستعلام البحث: {search_query}\\n\\nمحتوى الصفحة (أول 20000 حرف):\\n{page_text[:20000]}\\n\\n{prompt}\"}\n",
        "    ]\n",
        "    response = await call_openrouter_async(session, messages)\n",
        "    if response:\n",
        "        logger.debug(\"تم استخراج المعلومات ذات الصلة بنجاح.\")\n",
        "        return response.strip()\n",
        "    logger.warning(\"فشل استخراج المعلومات ذات الصلة.\")\n",
        "    return \"\"\n",
        "\n",
        "@timed\n",
        "async def get_new_search_queries_async(session: aiohttp.ClientSession, user_query: str, previous_search_queries: list, all_contexts: list) -> list:\n",
        "    \"\"\"\n",
        "    تحديد ما إذا كانت هناك حاجة لمزيد من استعلامات البحث بناءً على الاستعلام والمعلومات المجمعة.\n",
        "    تُعيد الدالة قائمة بايثون جديدة من الاستعلامات أو الرمز \"<done>\" في حال عدم الحاجة للمزيد.\n",
        "    \"\"\"\n",
        "    context_combined = \"\\n\".join(all_contexts)\n",
        "    prompt = (\n",
        "        \"أنت مساعد بحث تحليلي. بناءً على استعلام المستخدم واستعلامات البحث السابقة \"\n",
        "        \"والمعلومات المستخرجة، حدد ما إذا كانت هناك حاجة لمزيد من البحث. \"\n",
        "        \"إذا كان هناك حاجة، أعد حتى أربعة استعلامات بحث جديدة كقائمة بايثون. \"\n",
        "        \"إذا كان البحث مكتملًا، أجب بالرمز <done> فقط.\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"أنت مخطط بحث منهجي.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"استعلام المستخدم: {user_query}\\nاستعلامات البحث السابقة: {previous_search_queries}\\n\\nالمعلومات المستخرجة:\\n{context_combined}\\n\\n{prompt}\"}\n",
        "    ]\n",
        "    response = await call_openrouter_async(session, messages)\n",
        "    if response:\n",
        "        cleaned = response.strip()\n",
        "        if cleaned == \"<done>\":\n",
        "            logger.debug(\"أشار النموذج إلى انتهاء البحث (دون حاجة لاستعلامات جديدة).\")\n",
        "            return \"<done>\"\n",
        "        try:\n",
        "            new_queries = eval(cleaned)\n",
        "            if isinstance(new_queries, list):\n",
        "                logger.debug(\"تم توليد استعلامات بحث جديدة: %s\", new_queries)\n",
        "                return new_queries\n",
        "            else:\n",
        "                logger.warning(\"الرد لم يكن قائمة لاستعلامات البحث الجديدة: الرد = %s\", response)\n",
        "                return []\n",
        "        except Exception as e:\n",
        "            logger.error(\"خطأ في تحليل استعلامات البحث الجديدة باستخدام eval:\", exc_info=True)\n",
        "            return []\n",
        "    logger.warning(\"لم يتم الحصول على استجابة لتوليد استعلامات البحث الجديدة.\")\n",
        "    return []\n",
        "\n",
        "@timed\n",
        "async def generate_final_report_async(session: aiohttp.ClientSession, user_query: str, all_contexts: list) -> str:\n",
        "    \"\"\"\n",
        "    توليد التقرير النهائي باستخدام جميع المعلومات المجمعة.\n",
        "    يتم تجميع النصوص المفيدة وتحويلها لتقرير شامل ومنظم.\n",
        "    \"\"\"\n",
        "    context_combined = \"\\n\".join(all_contexts)\n",
        "    prompt = (\n",
        "        \"أنت كاتب تقارير خبير. بناءً على المعلومات المجمعة واستعلام المستخدم، \"\n",
        "        \"اكتب تقريراً شاملاً ومنظماً يجيب على الاستعلام بدقة دون تعليق زائد.\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"أنت كاتب تقارير ماهر.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"استعلام المستخدم: {user_query}\\n\\nالمعلومات المجمعة:\\n{context_combined}\\n\\n{prompt}\"}\n",
        "    ]\n",
        "    report = await call_openrouter_async(session, messages)\n",
        "    if report:\n",
        "        logger.debug(\"تم توليد التقرير النهائي بنجاح.\")\n",
        "    else:\n",
        "        logger.error(\"فشل توليد التقرير النهائي.\")\n",
        "    return report\n",
        "\n",
        "@timed\n",
        "async def process_link(session: aiohttp.ClientSession, link: str, user_query: str, search_query: str) -> str:\n",
        "    \"\"\"\n",
        "    معالجة رابط واحد: جلب محتوى الصفحة، تقييم فائدتها، واستخراج المعلومات ذات الصلة.\n",
        "    \"\"\"\n",
        "    logger.debug(\"بدء معالجة الرابط: %s\", link)\n",
        "    page_text = await fetch_webpage_text_async(session, link)\n",
        "    if not page_text:\n",
        "        logger.warning(\"لا يوجد محتوى في الصفحة: %s\", link)\n",
        "        return None\n",
        "    usefulness = await is_page_useful_async(session, user_query, page_text)\n",
        "    logger.debug(\"تقييم فائدة الصفحة %s: %s\", link, usefulness)\n",
        "    if usefulness == \"Yes\":\n",
        "        context = await extract_relevant_context_async(session, user_query, search_query, page_text)\n",
        "        if context:\n",
        "            logger.debug(\"تم استخراج معلومات من %s (المحتوى المقتطع: %s)\", link, context[:200])\n",
        "            return context\n",
        "    return None\n",
        "\n",
        "# ===========================================================\n",
        "# الدالة الرئيسية غير المتزامنة لتشغيل جميع عمليات البحث والتجميع\n",
        "# ===========================================================\n",
        "@timed\n",
        "async def async_main():\n",
        "    \"\"\"\n",
        "    الدالة الرئيسية التي تدير سير العمل الكامل:\n",
        "    - استلام استعلام البحث من المستخدم.\n",
        "    - توليد استعلامات البحث الأولية.\n",
        "    - إجراء حلقة بحث تكرارية لمعالجة النتائج.\n",
        "    - توليد التقرير النهائي وعرضه للمستخدم.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # استقبال استعلام البحث من المستخدم\n",
        "        user_query = input(\"أدخل استعلام البحث أو الموضوع: \").strip()\n",
        "        if not user_query:\n",
        "            logger.critical(\"لم يتم إدخال استعلام صالح. إنهاء البرنامج.\")\n",
        "            return\n",
        "\n",
        "        # استقبال الحد الأقصى للتكرارات مع التحقق من صحة الإدخال\n",
        "        iter_limit_input = input(\"أدخل الحد الأقصى للتكرارات (افتراضي 10): \").strip()\n",
        "        iteration_limit = int(iter_limit_input) if iter_limit_input.isdigit() else 10\n",
        "\n",
        "        aggregated_contexts = []  # لتجميع جميع المعلومات المفيدة من كل تكرار\n",
        "        all_search_queries = []   # لتجميع جميع استعلامات البحث المستخدمة\n",
        "        iteration = 0\n",
        "\n",
        "        # استخدام جلسة aiohttp مشتركة لكل الطلبات الشبكية\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            # ----- استعلامات البحث الأولية -----\n",
        "            new_search_queries = await generate_search_queries_async(session, user_query)\n",
        "            if not new_search_queries:\n",
        "                logger.critical(\"لم يتم توليد استعلامات بحث بواسطة النموذج. إنهاء البرنامج.\")\n",
        "                return\n",
        "            all_search_queries.extend(new_search_queries)\n",
        "\n",
        "            # ----- حلقة البحث التكرارية -----\n",
        "            while iteration < iteration_limit:\n",
        "                logger.info(\"=== التكرار %d ===\", iteration + 1)\n",
        "                iteration_contexts = []\n",
        "\n",
        "                # تنفيذ بحث SERPAPI لكل استعلام بشكل متزامن\n",
        "                search_tasks = [perform_search_async(session, query) for query in new_search_queries]\n",
        "                search_results = await asyncio.gather(*search_tasks)\n",
        "\n",
        "                # تجميع جميع الروابط الفريدة وربط كل رابط بالاستعلام الذي نتج عنه\n",
        "                unique_links = {}\n",
        "                for idx, links in enumerate(search_results):\n",
        "                    query = new_search_queries[idx]\n",
        "                    for link in links:\n",
        "                        if link not in unique_links:\n",
        "                            unique_links[link] = query\n",
        "\n",
        "                logger.info(\"تم تجميع %d روابط فريدة في هذا التكرار.\", len(unique_links))\n",
        "\n",
        "                # معالجة كل رابط بشكل متزامن: جلب المحتوى، تقييم الفائدة، واستخراج المعلومات\n",
        "                link_tasks = [\n",
        "                    process_link(session, link, user_query, unique_links[link])\n",
        "                    for link in unique_links\n",
        "                ]\n",
        "                link_results = await asyncio.gather(*link_tasks)\n",
        "\n",
        "                # تجميع المعلومات المفيدة من كل الرابط\n",
        "                for res in link_results:\n",
        "                    if res:\n",
        "                        iteration_contexts.append(res)\n",
        "\n",
        "                if iteration_contexts:\n",
        "                    aggregated_contexts.extend(iteration_contexts)\n",
        "                else:\n",
        "                    logger.warning(\"لم يتم العثور على معلومات مفيدة في هذا التكرار.\")\n",
        "\n",
        "                # ----- التحقق مما إذا كانت هناك حاجة لمزيد من استعلامات البحث -----\n",
        "                new_search_queries = await get_new_search_queries_async(session, user_query, all_search_queries, aggregated_contexts)\n",
        "                if new_search_queries == \"<done>\":\n",
        "                    logger.info(\"أشار النموذج إلى عدم الحاجة لمزيد من البحث.\")\n",
        "                    break\n",
        "                elif new_search_queries:\n",
        "                    logger.info(\"تم توليد استعلامات بحث جديدة: %s\", new_search_queries)\n",
        "                    all_search_queries.extend(new_search_queries)\n",
        "                else:\n",
        "                    logger.info(\"لم يتم توليد استعلامات بحث جديدة. إنهاء الحلقة.\")\n",
        "                    break\n",
        "\n",
        "                iteration += 1\n",
        "\n",
        "            # ----- توليد التقرير النهائي -----\n",
        "            logger.info(\"جاري توليد التقرير النهائي...\")\n",
        "            final_report = await generate_final_report_async(session, user_query, aggregated_contexts)\n",
        "            if final_report:\n",
        "                print(\"\\n==== التقرير النهائي ====\\n\")\n",
        "                print(final_report)\n",
        "            else:\n",
        "                logger.error(\"لم يتم توليد تقرير نهائي.\")\n",
        "\n",
        "    except Exception as main_exc:\n",
        "        logger.critical(\"حدث خطأ في الدالة الرئيسية async_main:\", exc_info=True)\n",
        "\n",
        "# ===========================================================\n",
        "# الدالة الرئيسية لتشغيل البرنامج\n",
        "# ===========================================================\n",
        "def main():\n",
        "    \"\"\"\n",
        "    الدالة الرئيسية التي تنفذ البرنامج باستخدام asyncio.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        asyncio.run(async_main())\n",
        "    except Exception as e:\n",
        "        logger.critical(\"حدث خطأ أثناء تشغيل البرنامج الرئيسي:\", exc_info=True)\n",
        "\n",
        "# نقطة البداية للتنفيذ\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# ===========================================================\n",
        "# ملاحظات إضافية للتطوير المستقبلي:\n",
        "# - تم تعيين مفتاح GEMINI_API_KEY مباشرة في الكود لتجنب مشكلات متغيرات البيئة.\n",
        "# - تمت إزالة نموذج anthropic/claude-3.5-haiku واعتماد gemini-1.5-pro حصرياً.\n",
        "# - تم استبدال مفتاح SERPAPI_API_KEY بمفتاح مجاني (أو تركه فارغاً) بحيث لا يعتمد على API مدفوع.\n",
        "# - تم تعزيز استخدام asyncio لتجميع المهام وتحسين سرعة الاستجابة.\n",
        "# - يُفضل إجراء اختبارات وحدة شاملة للتأكد من استقرار كل دالة.\n",
        "# - النظام مجهز لإعادة المحاولة تلقائياً عند فشل بعض الطلبات الشبكية.\n",
        "# - تم الحفاظ على كافة المعلومات السرية مثل API keys والـ tokens كما هي دون تعديل.\n",
        "# - تم التأكيد على عدم إضافة أي مزايا جديدة أو خدمات خارجية إضافية.\n",
        "# - الهدف النهائي هو تحقيق أقصى أداء MAP وتقديم تجربة استخدام لا غنى عنها للمستخدم.\n",
        "# ===========================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "46Q5XpapDJZT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}